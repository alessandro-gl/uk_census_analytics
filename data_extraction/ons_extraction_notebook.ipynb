{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98645eab-29e9-4fb3-a820-f09cc76c562b",
   "metadata": {},
   "source": [
    "# UK Census Data Extraction Pipeline\n",
    "**Extracting demographic data from ONS API to BigQuery**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10d44e93-e25c-4a11-8ee5-c52524810ceb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üè¥ UK CENSUS DATA PIPELINE\n",
      "======================================================================\n",
      "üìç Project: uk-census-portfolio\n",
      "üìÇ Target: raw_data.ons_census_raw\n",
      "======================================================================\n",
      "üìã Will extract 44 dimensions for 1 area type(s)\n",
      "   Total API calls: 44\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ This will take ~1-2 minutes. Continue? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üì• STEP 1: EXTRACTING FROM ONS API\n",
      "======================================================================\n",
      "[1/44] UR | wpc | activity_last_week\n",
      "[2/44] UR | wpc | age_arrival_uk_23a\n",
      "[3/44] UR | wpc | alternative_address_indicator\n",
      "[4/44] UR | wpc | country_of_birth_190a\n",
      "‚ö†Ô∏è  Skipping wpc-country_of_birth_190a: Dataset too large for API\n",
      "[5/44] UR | wpc | country_of_birth_60a\n",
      "[6/44] UR | wpc | disability\n",
      "[7/44] UR | wpc | economic_activity_status_12a\n",
      "[8/44] UR | wpc | english_proficiency\n",
      "[9/44] UR | wpc | ethnic_group_tb_20b\n",
      "[10/44] UR | wpc | has_ever_worked\n",
      "[11/44] UR | wpc | health_in_general\n",
      "[12/44] UR | wpc | highest_qualification\n",
      "[13/44] UR | wpc | hours_per_week_worked\n",
      "[14/44] UR | wpc | industry_current_88a\n",
      "[15/44] UR | wpc | industry_former_17a\n",
      "[16/44] UR | wpc | is_carer\n",
      "[17/44] UR | wpc | legal_partnership_status\n",
      "[18/44] UR | wpc | living_arrangements_11a\n",
      "[19/44] UR | wpc | main_language_23a\n",
      "[20/44] UR | wpc | main_language_detailed_26a\n",
      "[21/44] UR | wpc | migrant_ind\n",
      "[22/44] UR | wpc | multi_passports\n",
      "[23/44] UR | wpc | national_identity_all\n",
      "[24/44] UR | wpc | national_identity_detailed\n",
      "[25/44] UR | wpc | ns_sec\n",
      "[26/44] UR | wpc | occupation_current_105a\n",
      "[27/44] UR | wpc | occupation_former\n",
      "[28/44] UR | wpc | passports_all_52a\n",
      "[29/44] UR | wpc | place_of_work_ind\n",
      "[30/44] UR | wpc | religion_tb\n",
      "[31/44] UR | wpc | residence_length_6b\n",
      "[32/44] UR | wpc | resident_age_101a\n",
      "[33/44] UR | wpc | sex\n",
      "[34/44] UR | wpc | transport_to_workplace\n",
      "[35/44] UR | wpc | uk_armed_forces\n",
      "[36/44] UR | wpc | welsh_skills_all\n",
      "[37/44] UR | wpc | welsh_skills_read\n",
      "[38/44] UR | wpc | welsh_skills_speak\n",
      "[39/44] UR | wpc | welsh_skills_understand\n",
      "[40/44] UR | wpc | welsh_skills_write\n",
      "[41/44] UR | wpc | welsh_speaking_3_plus\n",
      "[42/44] UR | wpc | welsh_speaking_dependent_child\n",
      "[43/44] UR | wpc | workplace_travel\n",
      "[44/44] UR | wpc | year_arrival_uk\n",
      "\n",
      "‚úÖ Extraction complete: 486,477 rows\n",
      "\n",
      "======================================================================\n",
      "üíæ STEP 2: SAVING LOCAL BACKUP\n",
      "======================================================================\n",
      "‚úÖ Saved to: data/wpc_results.csv\n",
      "   üìä File size: 81.85 MB\n",
      "\n",
      "======================================================================\n",
      "üóÑÔ∏è  STEP 3: SETTING UP BIGQUERY\n",
      "======================================================================\n",
      "‚úÖ Dataset uk-census-portfolio.raw_data already exists\n",
      "\n",
      "======================================================================\n",
      "‚òÅÔ∏è  STEP 4: LOADING TO BIGQUERY\n",
      "======================================================================\n",
      "üì§ Loading 486,477 rows to uk-census-portfolio.raw_data.ons_census_raw...\n",
      "‚úÖ Loaded 486,477 rows to BigQuery\n",
      "   üìä Table size: 86.84 MB\n",
      "   üîó View at: https://console.cloud.google.com/bigquery?project=uk-census-portfolio&p=uk-census-portfolio&d=raw_data&t=ons_census_raw&page=table\n",
      "\n",
      "======================================================================\n",
      "‚úÖ PIPELINE COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "üìä Data Summary:\n",
      "   ‚Ä¢ Rows loaded: 486,477\n",
      "   ‚Ä¢ Unique areas: 573\n",
      "   ‚Ä¢ Variables: 43\n",
      "\n",
      "üîç Next Steps:\n",
      "   1. View data in BigQuery console:\n",
      "      https://console.cloud.google.com/bigquery?project=uk-census-portfolio\n",
      "\n",
      "   2. Test query:\n",
      "      SELECT area_name, variable_type_id, COUNT(*) as row_count\n",
      "      FROM `uk-census-portfolio.raw_data.ons_census_raw`\n",
      "      GROUP BY 1, 2\n",
      "      ORDER BY 3 DESC\n",
      "      LIMIT 10;\n",
      "\n",
      "   3. Set up dbt project:\n",
      "      dbt init uk_census_analytics\n",
      "\n",
      "   4. Create staging model:\n",
      "      models/staging/stg_ons_census.sql\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "import requests\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# ============================================\n",
    "# CONFIG\n",
    "# ============================================\n",
    "API_ROOT = \"https://api.beta.ons.gov.uk/v1/population-types\"\n",
    "\n",
    "# TODO: Replace with YOUR actual project ID from GCP Console\n",
    "PROJECT_ID = \"uk-census-portfolio\"  # ‚Üê Change this to your actual project ID\n",
    "DATASET_ID = \"raw_data\"\n",
    "TABLE_ID = \"ons_census_raw\"\n",
    "\n",
    "def fetch_observations(population_type: str, area_type: str, dimension: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetch one dimension for one area_type, return flattened DataFrame or empty.\"\"\"\n",
    "    url = f\"{API_ROOT}/{population_type}/census-observations\"\n",
    "    try:\n",
    "        r = requests.get(\n",
    "            url, \n",
    "            params={\"area-type\": area_type, \"dimensions\": dimension}, \n",
    "            timeout=30\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        payload = r.json()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if e.response.status_code == 403:\n",
    "            print(f\"‚ö†Ô∏è  Skipping {area_type}-{dimension}: Dataset too large for API\")\n",
    "        else:\n",
    "            print(f\"‚ùå Request failed for {area_type}-{dimension}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error for {area_type}-{dimension}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    obs = payload.get(\"observations\", [])\n",
    "    if not obs:\n",
    "        print(f\"‚ÑπÔ∏è  No observations for {area_type}-{dimension}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    records = []\n",
    "    for o in obs:\n",
    "        dims = o.get(\"dimensions\", [])\n",
    "        if len(dims) < 2:\n",
    "            continue\n",
    "        area_dim, var_dim = dims[0], dims[1]\n",
    "        records.append({\n",
    "            \"area_type_name\": area_dim.get(\"dimension\"),\n",
    "            \"area_type_id\": area_dim.get(\"dimension_id\"),\n",
    "            \"area_name\": area_dim.get(\"option\"),\n",
    "            \"area_id\": area_dim.get(\"option_id\"),\n",
    "            \"variable_type_name\": var_dim.get(\"dimension\"),\n",
    "            \"variable_type_id\": var_dim.get(\"dimension_id\"),\n",
    "            \"variable_name\": var_dim.get(\"option\"),\n",
    "            \"variable_id\": var_dim.get(\"option_id\"),\n",
    "            \"observation\": o.get(\"observation\"),\n",
    "        })\n",
    "    return pd.DataFrame.from_records(records)\n",
    "\n",
    "def collect_all(area_types: List[str], population_type: str, dimensions: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Loop area types and dimensions; concatenate non-empty results.\"\"\"\n",
    "    frames = []\n",
    "    total = len(area_types) * len(dimensions)\n",
    "    current = 0\n",
    "    \n",
    "    for a in area_types:\n",
    "        for d in dimensions:\n",
    "            current += 1\n",
    "            print(f\"[{current}/{total}] {population_type} | {a} | {d}\")\n",
    "            df = fetch_observations(population_type, a, d)\n",
    "            if not df.empty:\n",
    "                frames.append(df)\n",
    "    \n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "def ensure_bigquery_dataset(project_id: str, dataset_id: str, location: str = \"EU\"):\n",
    "    \"\"\"Create BigQuery dataset if it doesn't exist.\"\"\"\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    dataset_ref = f\"{project_id}.{dataset_id}\"\n",
    "    \n",
    "    try:\n",
    "        client.get_dataset(dataset_ref)\n",
    "        print(f\"‚úÖ Dataset {dataset_ref} already exists\")\n",
    "    except Exception:\n",
    "        dataset = bigquery.Dataset(dataset_ref)\n",
    "        dataset.location = location\n",
    "        dataset.description = \"Raw census data from ONS API (Portfolio Project)\"\n",
    "        client.create_dataset(dataset, timeout=30)\n",
    "        print(f\"‚úÖ Created dataset {dataset_ref}\")\n",
    "\n",
    "def load_to_bigquery(df: pd.DataFrame, project_id: str, dataset_id: str, table_id: str):\n",
    "    \"\"\"Load DataFrame to BigQuery with explicit schema.\"\"\"\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "    \n",
    "    # Define schema explicitly (best practice)\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"area_type_name\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"area_type_id\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"area_name\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"area_id\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"variable_type_name\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"variable_type_id\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"variable_name\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"variable_id\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"observation\", \"INTEGER\"),\n",
    "    ]\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=schema,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # Overwrite on each run\n",
    "    )\n",
    "    \n",
    "    print(f\"üì§ Loading {len(df):,} rows to {table_ref}...\")\n",
    "    job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
    "    job.result()  # Wait for completion\n",
    "    \n",
    "    # Verify and show stats\n",
    "    table = client.get_table(table_ref)\n",
    "    print(f\"‚úÖ Loaded {table.num_rows:,} rows to BigQuery\")\n",
    "    print(f\"   üìä Table size: {table.num_bytes / (1024**2):.2f} MB\")\n",
    "    print(f\"   üîó View at: https://console.cloud.google.com/bigquery?project={project_id}&p={project_id}&d={dataset_id}&t={table_id}&page=table\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üè¥ UK CENSUS DATA PIPELINE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"üìç Project: {PROJECT_ID}\")\n",
    "    print(f\"üìÇ Target: {DATASET_ID}.{TABLE_ID}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load metadata from your ons_words.json\n",
    "    try:\n",
    "        with open(\"ons_words.json\", \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Error: ons_words.json not found in current directory\")\n",
    "        print(\"   Make sure you're running this script from the same folder as ons_words.json\")\n",
    "        exit(1)\n",
    "\n",
    "    # Configure extraction\n",
    "    population_type = \"UR\"  # Usual Residents\n",
    "    area_types = [\"wpc\"]    # Westminster Parliamentary Constituencies (start with this)\n",
    "    dimensions = meta[\"DIMENSIONS_BY_POPULATION_TYPE\"][population_type]\n",
    "    \n",
    "    print(f\"üìã Will extract {len(dimensions)} dimensions for {len(area_types)} area type(s)\")\n",
    "    print(f\"   Total API calls: {len(dimensions) * len(area_types)}\")\n",
    "    \n",
    "    # Ask for confirmation before proceeding\n",
    "    user_input = input(\"\\n‚è≥ This will take ~1-2 minutes. Continue? (y/n): \")\n",
    "    if user_input.lower() != 'y':\n",
    "        print(\"‚ùå Cancelled by user\")\n",
    "        exit(0)\n",
    "\n",
    "    # Step 1: Extract from API\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üì• STEP 1: EXTRACTING FROM ONS API\")\n",
    "    print(\"=\" * 70)\n",
    "    combined_df = collect_all(area_types, population_type, dimensions)\n",
    "    \n",
    "    if combined_df.empty:\n",
    "        print(\"‚ùå No data extracted. Exiting.\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Extraction complete: {len(combined_df):,} rows\")\n",
    "\n",
    "    # Step 2: Save locally (backup)\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üíæ STEP 2: SAVING LOCAL BACKUP\")\n",
    "    print(\"=\" * 70)\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    backup_path = \"data/wpc_results.csv\"\n",
    "    combined_df.to_csv(backup_path, index=False)\n",
    "    file_size_mb = os.path.getsize(backup_path) / (1024**2)\n",
    "    print(f\"‚úÖ Saved to: {backup_path}\")\n",
    "    print(f\"   üìä File size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "    # Step 3: Create BigQuery dataset if needed\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üóÑÔ∏è  STEP 3: SETTING UP BIGQUERY\")\n",
    "    print(\"=\" * 70)\n",
    "    try:\n",
    "        ensure_bigquery_dataset(PROJECT_ID, DATASET_ID, location=\"EU\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create dataset: {e}\")\n",
    "        print(\"\\nüí° Common fixes:\")\n",
    "        print(\"   1. Make sure PROJECT_ID is correct in the script\")\n",
    "        print(\"   2. Run: gcloud auth application-default login\")\n",
    "        print(\"   3. Enable BigQuery API: gcloud services enable bigquery.googleapis.com\")\n",
    "        exit(1)\n",
    "\n",
    "    # Step 4: Load to BigQuery\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚òÅÔ∏è  STEP 4: LOADING TO BIGQUERY\")\n",
    "    print(\"=\" * 70)\n",
    "    try:\n",
    "        load_to_bigquery(combined_df, PROJECT_ID, DATASET_ID, TABLE_ID)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load data: {e}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Success summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ PIPELINE COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nüìä Data Summary:\")\n",
    "    print(f\"   ‚Ä¢ Rows loaded: {len(combined_df):,}\")\n",
    "    print(f\"   ‚Ä¢ Unique areas: {combined_df['area_name'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Variables: {combined_df['variable_type_id'].nunique()}\")\n",
    "    \n",
    "    print(f\"\\nüîç Next Steps:\")\n",
    "    print(f\"   1. View data in BigQuery console:\")\n",
    "    print(f\"      https://console.cloud.google.com/bigquery?project={PROJECT_ID}\")\n",
    "    print(f\"\\n   2. Test query:\")\n",
    "    print(f\"      SELECT area_name, variable_type_id, COUNT(*) as row_count\")\n",
    "    print(f\"      FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\")\n",
    "    print(f\"      GROUP BY 1, 2\")\n",
    "    print(f\"      ORDER BY 3 DESC\")\n",
    "    print(f\"      LIMIT 10;\")\n",
    "    print(f\"\\n   3. Set up dbt project:\")\n",
    "    print(f\"      dbt init uk_census_analytics\")\n",
    "    print(f\"\\n   4. Create staging model:\")\n",
    "    print(f\"      models/staging/stg_ons_census.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08877418-12f1-4e15-a41d-4efce2259a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8129f003-fc6a-4eba-b2f5-1e746f8dbfb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
